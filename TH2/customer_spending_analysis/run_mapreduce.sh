#!/bin/bash
"""
Script ch·∫°y Customer Spending Analysis tr√™n Hadoop
"""

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$SCRIPT_DIR"
SRC_DIR="$PROJECT_DIR/src"
DATA_DIR="$PROJECT_DIR/data"

echo "üöÄ Customer Spending Analysis - Hadoop MapReduce"
echo "==============================================="

# Ki·ªÉm tra Hadoop
if ! command -v hadoop &> /dev/null; then
    echo "‚ùå Hadoop kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y. Ki·ªÉm tra HADOOP_HOME v√† PATH"
    exit 1
fi

# Ki·ªÉm tra file input
INPUT_FILE="$DATA_DIR/input_combined.txt"
if [ ! -f "$INPUT_FILE" ]; then
    echo "‚ùå Kh√¥ng t√¨m th·∫•y file input: $INPUT_FILE"
    echo "üí° Ch·∫°y data generator tr∆∞·ªõc:"
    echo "   cd $SRC_DIR && python3 data_generator.py"
    exit 1
fi

# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n HDFS
HDFS_INPUT_DIR="/user/$(whoami)/customer_spending/input"
HDFS_OUTPUT_DIR="/user/$(whoami)/customer_spending/output"
LOCAL_OUTPUT_DIR="$PROJECT_DIR/output"

echo "üìÇ Chu·∫©n b·ªã d·ªØ li·ªáu tr√™n HDFS..."

# X√≥a th∆∞ m·ª•c c≈© n·∫øu c√≥
hdfs dfs -rm -r -f "$HDFS_INPUT_DIR" "$HDFS_OUTPUT_DIR"

# T·∫°o th∆∞ m·ª•c input tr√™n HDFS
hdfs dfs -mkdir -p "$HDFS_INPUT_DIR"

# Upload file input l√™n HDFS
hdfs dfs -put "$INPUT_FILE" "$HDFS_INPUT_DIR/"

echo "‚úÖ ƒê√£ upload d·ªØ li·ªáu l√™n HDFS"
echo "üìä S·ªë records: $(wc -l < "$INPUT_FILE")"

# Ch·∫°y Hadoop MapReduce job
echo ""
echo "üîÑ Ch·∫°y Hadoop MapReduce job..."

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -files "$SRC_DIR/mapper.py","$SRC_DIR/reducer.py" \
    -mapper "python3 mapper.py" \
    -reducer "python3 reducer.py" \
    -input "$HDFS_INPUT_DIR/input_combined.txt" \
    -output "$HDFS_OUTPUT_DIR"

if [ $? -ne 0 ]; then
    echo "‚ùå Hadoop MapReduce job th·∫•t b·∫°i!"
    exit 1
fi

echo "‚úÖ Hadoop MapReduce job ho√†n th√†nh!"

# T·∫£i k·∫øt qu·∫£ v·ªÅ local
echo ""
echo "üì• T·∫£i k·∫øt qu·∫£ v·ªÅ local..."
mkdir -p "$LOCAL_OUTPUT_DIR"

hdfs dfs -get "$HDFS_OUTPUT_DIR/part-00000" "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv.tmp"

# Th√™m header CSV
{
    echo "Cust_ID,Customer_Name,Total_Spending,Transaction_Count"
    cat "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv.tmp"
} > "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv"

rm -f "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv.tmp"

# Hi·ªÉn th·ªã k·∫øt qu·∫£
echo ""
echo "üìä K·∫æT QU·∫¢ CUSTOMER SPENDING ANALYSIS"
echo "===================================="
echo ""

# Format output ƒë·∫πp t·ª´ CSV
python3 -c "
import csv
import sys

with open('$LOCAL_OUTPUT_DIR/customer_spending_summary.csv', 'r') as f:
    reader = csv.reader(f)
    header = next(reader)
    
    # Print header
    print(f'{header[0]:<10} {header[1]:<20} {header[2]:<15} {header[3]:<18}')
    print('-' * 65)
    
    # Print data rows
    for row in reader:
        print(f'{row[0]:<10} {row[1]:<20} \${row[2]:<14} {row[3]:<18}')
"

echo ""
echo "üìÅ File k·∫øt qu·∫£: $LOCAL_OUTPUT_DIR/customer_spending_summary.csv"

# Th·ªëng k√™ t·ªïng quan t·ª´ CSV
TOTAL_CUSTOMERS=$(($(wc -l < "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv") - 1))  # Tr·ª´ header
TOTAL_SPENDING=$(awk -F',' 'NR>1 {sum += $3} END {printf "%.2f", sum}' "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv")
TOTAL_TRANSACTIONS=$(awk -F',' 'NR>1 {sum += $4} END {print sum}' "$LOCAL_OUTPUT_DIR/customer_spending_summary.csv")

echo ""
echo "üìà TH·ªêNG K√ä T·ªîNG QUAN"
echo "==================="
echo "‚Ä¢ T·ªïng s·ªë kh√°ch h√†ng: $TOTAL_CUSTOMERS"
echo "‚Ä¢ T·ªïng s·ªë giao d·ªãch: $TOTAL_TRANSACTIONS"
echo "‚Ä¢ T·ªïng doanh thu: \$${TOTAL_SPENDING}"
echo "‚Ä¢ Trung b√¨nh chi ti√™u/kh√°ch h√†ng: \$$(echo "scale=2; $TOTAL_SPENDING / $TOTAL_CUSTOMERS" | bc -l)"

echo ""
echo "‚úÖ Ho√†n th√†nh Customer Spending Analysis tr√™n Hadoop!"

# Hi·ªÉn th·ªã HDFS info
echo ""
echo "üóÇÔ∏è  HDFS Paths:"
echo "‚Ä¢ Input: $HDFS_INPUT_DIR"
echo "‚Ä¢ Output: $HDFS_OUTPUT_DIR"
